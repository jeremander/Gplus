\documentclass[11pt, oneside, fleqn]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper, margin=1.1in}  
%\geometry{landscape}                		% Activate for for rotated page geometry
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{enumerate}

\theoremstyle{plain} 
\newtheorem{theorem}{Theorem} 
\newtheorem{lemma}{Lemma} 

\newcommand{\given}{\; | \;}
\newcommand{\st}{\, : \,}
\newcommand{\reals}{\mathbb{R}}
\renewcommand{\arraystretch}{1.5}


%\title{}
%\author{Jeremy Silver}
%\date{Tues. 10/27/14}							% Activate to display a given date or no date

\begin{document}
%\maketitle
\section{Attribute distributions on undirected graphs}
Suppose we have an undirected graph $G = (V, E)$ a set $A$ of attributes, and an attribute assignment function $\phi \st V \rightarrow A$ mapping vertices to their attributes.  For simplicity, we may assume that $A$ contains a ``missing'' attribute token to cover the case when the attribute of a vertex is unknown or missing.  We wish to compute a similarity score between pairs of attributes that measures their propensity to occur together in edges in the graph.  In other words, we seek a similarity function $s \st A \times A \rightarrow \reals$ for which greater values of $s(a, b)$ indicate greater likelihood of co-occurrence of $a$ and $b$.  Furthermore, $s$ should be symmetric, due to the undirected nature of the graph.

Following \cite{nameLocClustering}, we use \textit{pointwise mutual information} (PMI) as a similarity score.  PMI is defined as:
$$ PMI(a, b) = \log \frac{P(a, b)}{P(a) P(b)}. $$
This measures the relative co-occurrence likelihood of $a$ and $b$ with respect to the assumption of independence.  If $a$ and $b$ occur independently of each other, then the PMI will be 0.  The PMI will be positive (negative) if $a$ and $b$ occur more (less) frequently together than if they had been chosen independently.

$P(a, b)$ is defined as the probability that an edge in $E$ will have attributes $\{a, b\}$.  In the graph context, it is tempting to think of $P(a)$ as the probability that a vertex in $V$ has attribute $a$, but this is incorrect.  $P(a)$ is in fact the marginal probability that $a$ occurs as one of the attributes of an edge $(u, v) \in E$:
$$ P(a) = P \displaystyle \left ( \bigcup_{b \in A} (\phi(u), \phi(v)) = (a, b) \right ) = \sum_{b \in A} P(a, b). $$

\section{Normalization of PMI}
Assuming that $P(a) > 0$ for all $a \in A$, the range of values for PMI$(a, b)$ is
$$(-\infty, \, \min(-\log P(a), -\log P(b))]. $$
This is because PMI can also be written as $\log \frac{P(a \given b)}{P(a)}$ or $\log \frac{P(b \given a)}{P(b)}$, which are respectively maximized when $P(a \given b) = 1$ or $P(b \given a) = 1$, yielding $\min(-\log P(a), -\log P(b))$ as an upper bound.  In the event that $a$ and $b$ only occur with each other, $P(a, b) = P(a) = P(b)$, and the upper bound reduces to $-\log P(a, b)$.  Noting that this upper bound varies with $a$ and $b$, we propose two alternative normalizations of PMI in the following table.

\begin{center}
\begin{tabular}{|l|c|c|} 
 \hline
\multicolumn{1}{|c|}{\textbf{Similarity}} & \textbf{Range} & \textbf{Independence value} \\ 
\hline
$\text{PMI}(a, b) =  \log \frac{P(a, b)}{P(a) P(b)}$ & $(-\infty, -\log P(a, b)]$ & $0$ \\ 
$\text{NPMI1}(a, b) = \frac{\log P(a, b) - \text{PMI}(a, b)}{2 \log P(a, b)} = \frac{\log(P(a) P(b))}{2 \log P(a, b)}$ & $[0, 1]$ & $\frac{1}{2}$  \\
$\text{NPMI2}(a, b) = -\frac{\log(1 - \text{NPMI1}(a, b))}{\log 2}$ & $[0, \infty)$ & $1$ \\
\hline
\end{tabular}
\end{center}
Depending on the application, using one of these may be preferable to using the basic PMI.  In particular, NPMI1 is nonnegative and bounded, both of which may be desirable properties.  Furthermore, each of these similarity scores can be readily converted to dissimilarity scores using appropriate shifts and reflections.

\section{Empirical PMI}
The formula for PMI treats the attributes on the graph vertices as random variables, but we wish to estimate the PMI from an actual graph whose attributes are known.  To do so, we use the sample frequencies of attribute pairs in the edge set.  Let
$$ f_{a,b} = | \{ \{u, v\} \in E \st \{a, b\} = \{\phi(u), \phi(v)\} \} |, \quad f_a = | \{ \{u, v\} \in E \st a \in \{\phi(u), \phi(v)\} \} |. $$
Then we estimate $P(a, b)$ by $\hat{P}(a, b) = \frac{f_{a,b}}{|E|}$, and $P(a)$ by $\hat{P}(a) = \frac{f_a}{|E|}$.  We can then use $\hat{P}$ in place of $P$ to compute the estimated PMI.  Note that by definition, $\sum_{a,b} f_{a,b} = |E|$, and $\sum_b f_{a,b} = f_a$ for all $a$, meaning that $\hat{P}$ is a properly normalized probability.

As is often the case with real data, $G$ will be sparse, and $A$ may be a large set, many of whose elements may only be represented by just a few edges in $E$.  In such cases, $f_{a,b} = 0$ for a vast majority of $a, b$ pairs, but assigning these pairs an empirical probability of 0 will drastically underrepresent them.  To give them positive probability, we smooth the counts by adding a constant $\delta > 0$ to each $f_{a,b}$.  This gives the following smoothed versions of the empirical probabilities:
$$ \hat{P}(a, b) = \frac{f_{a,b} + \delta}{|E| + \delta \binom{|A| + 1}{2}}, \quad \hat{P}(a) = \frac{f_a + \delta |A|}{|E| + \delta \binom{|A| + 1}{2}}. $$
We see that $\hat{P}(a, b)$ has the appropriate normalization because the set of attribute pairs is of size $\binom{|A| + 1}{2}$, since there are $\binom{|A|}{2}$ pairs of distinct attributes and $|A|$ pairs of identical attributes.  Having $\delta > 0$ guarantees that all probabilities are positive, and increasing $\delta$ distorts the distribution closer to the uniform distribution on attribute pairs.

\section{Sparse representation of Empirical PMI}
Suppose we want to represent the empirical PMI matrix in a sparse way.  Without smoothing, we could compute the empirical PMI for only the attribute pairs $\{a, b\}$ for which $f_{a,b} > 0$, so that we have $O(|E|)$ computations instead of $O(|A|^2)$.  We also only have to store this many values instead of the full $|A| \times |A|$ matrix.  In practice, it may be best to use NPMI1 or NPMI2 so that all un-represented attribute pairs have similarity 0 instead of $-\infty$.

If we want to use the $\delta$-smoothing, things get a little more complicated.  The empirical PMI matrix is not just a sum of a sparse matrix and a constant matrix (as would be the case for the counts alone) due to the $\hat{P}(a) \hat{P}(b)$ in the denominator.  However, we can represent the matrix in the following way.

Let $F$ be the matrix $F_{i,j} = \log(f_{a_i,a_j} + \delta) - \log(\delta)$, where $a_1, a_2, \ldots, a_{|A|}$ is some canonical ordering of the attributes.  Note that $F_{i,j} = 0$ whenever $f_{a_i,a_j} = 0$, meaning we can represent $F$ sparsely when the pairwise counts are sparse.  Now let $\textbf{v}$ be the vector for which $v_i = \log(f_{a_i} + \delta |A|)$, let $\Delta = \log(\delta) + \log(|E| + \delta \binom{|A| + 1}{2})$.  Denoting by $E$ the matrix of empirical PMIs, we have
$$ E = F + \Delta \mathbf{1} \mathbf{1}^T - \mathbf{v} \mathbf{1}^T - \mathbf{1} \mathbf{v}^T, $$
where $\mathbf{1}$ is the vector of all 1's.  So $E$ can be represented as a sum of a sparse matrix and three rank-one matrices.  

The sum representation of the empirical PMI is licensed by the linearity of PMI with respect to the log-probabilities.  Such linearity does not exist for the normalized versions of PMI, so a different kind of smoothing would be preferable in which normalized PMIs are computed for all nonzero-frequency attribute pairs, and then a small constant is added to all the values after normalization.


\begin{thebibliography}{1}
\bibitem{nameLocClustering} 
Bergsma, Shane; Dredze, Mark; Van Durme, Benjamin; Wilson, Theresa; and Yarowsky, David.  2013.  Broadly Improving User Classification via Communication-Based Name and Location Clustering on Twitter.  In \textit{NAACL}.
\end{thebibliography}

\end{document}  